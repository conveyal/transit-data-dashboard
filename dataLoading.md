# Loading data to the dashboard

This is the procedure I used to create the public-facing Transit Data Dashboard, using this software and data from a variety of places.

First off, you want to load several pieces of static data into your database. You'll want to load the National Transit Database by following the instructions in the README of this repository, and then load the Census Urban Areas also by following the instructions in the README. Then, you'll want to visit `/api/mapper/mapAgenciesByUzaAndMerge?commit=true` to merge connected urban areas.

Next, you want to set up your application-context.xml with an updater factory and a GTFS Data Exchange updater, like so:

```xml
&lt;bean id="updaters" class="updaters.UpdaterFactory"&gt;
  &lt;property name="updaters"&gt;
    &lt;list&gt;
      &lt;bean class="updaters.GtfsDataExchangeUpdater"&gt;
        &lt;property name="storer"&gt;
          &lt;bean class="updaters.FileFeedStorer"&gt;
            &lt;property name="path" value="/extra/gtfs" /&gt;
          &lt;/bean&gt;
        &lt;/property&gt;
      &lt;/bean&gt;
    &lt;/list&gt;
  &lt;/property&gt;

  &lt;property name="hooks"&gt;
    &lt;list&gt;
      &lt;bean class="updaters.LoggingUpdaterHook" /&gt;
    &lt;/list&gt;
  &lt;/property&gt;
&lt;/bean&gt;
```

Now, access `/api/mapper/fetchGtfs`. Very quickly, a JSON response will be returned saying `{"status":"running"}`, which means that the job has started. This will take quite some time, depending on the speed of your Internet and the system resources you have available (I was able to pull this off with an EC2 small instance running Ubuntu 12.04, with almost all of the RAM dedicated to Play! (`play run -Xmx1524m` or so).

Once that has completed, you can access the admin interface at `/api/admin/index` and figure out what to do about feeds that were not automatically mapped (also, `/admin/mapfeeds.html` will be useful, and `/api/gtfsfeeds/feedsNoAgencies` will too. This whole situation is in need of improvement; specifically, clicking on the unmapped agency button in the admin interface should do something).

Also, `/api/mapper/mapFeedsWithNoAgencies` will create agencies for all unmapped feeds, and attempt to give them metro areas. So, once you've manually matched everything that you can, run this and it will prepare your DB for the next step.

You'll then want to click 'Agencies matching multiple metro areas' and decide whether to split the agencies to all of the metro areas (i.e. all metro areas will contain the agency, but they will still be built as separate graphs and be otherwise separate) or to merge the metro areas. If you need something more complicated, you can merge them then split them, as described in the README.

`/api/mapper/autonameMetroAreas` should be used liberally throughout this process to ensure that metros have semireasonable names. Bear in mind that the names are autogenerated and the algorithm reflects the largest part of the metro, not necessarily the whole thing. For instance, in my instance, the Northeast Corridor of the United States is called something along the lines of New York, NY, when it extends from North Carolina to Massachusetts.

The hardest part is mapping the Google GTFS data, although there are a plethora of tools to help one with this. First, you'll want to load the Google GTFS data as described in the README. Watch it as it loads and take note of mismatched agencies (ignore unmatched ones for now). Then, go to the main admin page and first click on the 'unmatched metro areas' link. Proceed through the page as directed, matching each of Google's metro areas to one of ours. Keep an eye on the server WARN log output; it will tell you what additional agencies automatch, you should ensure it makes no mistakes.

Once all of the metros are matched (if you leave some unmatched, resulting agencies will also not match a metro), proceed to the admin screen again. Go to the 'unmatched GTFS providers' section and choose an agency for each of Google's agencies, or create a new agency.

At the end, there may be a little manual cleanup of the data through `psql` or PGAdmin. Make sure you run a pg_dump first.